{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: En este Notebook se trabajaran las funciones y las optimizaciones y los mejores resultados de cada una de las funciones es decir las versiones optimizadas serán las que se colocarán el los script de python. Dentro de este Notebook utilice time para determinar el tiempo de ejecución y poder optimizar y adicionamente utilicé memory-profile para el uso de la memoria y coloqué los resultados. Sim enbargio en los script de python tabn solo coloque la función para determinar lo que se solicitaba."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIMER PUNTO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de tiempo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las librerías necesarias para resolver el problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos realizar el la lectura de la ruta y luego leer el archivo y pasarlo a un Dataframe de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'farmers-protest-tweets-2021-2-4.json'\n",
    "dataframe = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora convertimos la columna 'date' a formato de fecha y eliminamos la hora para que solo se tenga en cuenta el día\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['date'] = pd.to_datetime(dataframe['date'])\n",
    "dataframe['date'] = dataframe['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego extraemos el 'username' de la columna user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['username'] = dataframe['user'].apply(lambda x: x['username'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos las columnas relevantes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns = ['date', 'username']\n",
    "dataframe = dataframe[filtered_columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora agrupamos por fecha y usuario y contamos la cantidad de tweets y encontramos las top 10 fechas con más tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = dataframe.groupby(['date', 'username']).size().reset_index(name='tweet_count')\n",
    "top_10_dates = grouped_data.groupby('date')['tweet_count'].sum().nlargest(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos ahora crear para almacenar los resultados en el formato en que se solicita la respuesta\n",
    "Filtramos para date_data es decir para las fechas específicas y con este filtro encontramos el usuario con mas publiaciones para la fecha. Por último agreagamos estos resultados a la tupla con el formato requerido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_result = []\n",
    "\n",
    "for date, tweet_count in top_10_dates.items():\n",
    "    date_data = grouped_data[grouped_data['date'] == date]\n",
    "    \n",
    "    top_user = date_data.loc[date_data['tweet_count'].idxmax()]\n",
    "    \n",
    "    formatted_result.append((datetime.datetime.strptime(date, '%Y-%m-%d').date(), top_user['username']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos la información resultante y verificamos que el resultado se encuentra en el formato solicitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(formatted_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la función TIME:\n",
    "### def q1_time(file_path: str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que tenemos todos los pasos ahora unificamos todo dentro de una función, agregamos start_time para que mida el tiempo de ejecución y lo imprima al ejecutar el codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date'])\n",
    "    dataframe['date'] = dataframe['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    dataframe['username'] = dataframe['user'].apply(lambda x: x['username'])\n",
    "\n",
    "    filtered_columns = ['date', 'username']\n",
    "    dataframe = dataframe[filtered_columns]\n",
    "\n",
    "    grouped_data = dataframe.groupby(['date', 'username']).size().reset_index(name='tweet_count')\n",
    "    top_10_dates = grouped_data.groupby('date')['tweet_count'].sum().nlargest(10)\n",
    "\n",
    "\n",
    "    formatted_result = []\n",
    "\n",
    "    for date, tweet_count in top_10_dates.items():\n",
    "        date_data = grouped_data[grouped_data['date'] == date]\n",
    "        top_user = date_data.loc[date_data['tweet_count'].idxmax()]\n",
    "        formatted_result.append((datetime.datetime.strptime(date, '%Y-%m-%d').date(), top_user['username']))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución: {execution_time} segundos\")\n",
    "\n",
    "    return formatted_result\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la prueba del codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 19.12430191040039 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para optimizar el tiempo un poco más en lugar de usar una función lambda en apply para extraer el 'username', se utilizará directamente la indexación de diccionario.\n",
    "Del mismo modo se filtrarán las columnas relevantes antes de realizar cualquier procesamiento adicional para minizar las iteraciones y por último en lugar de iterar sobre las top 10 fechas y filtrar los datos en cada iteración, se procesará los datos una vez y luego se iterará sobre ellos para encontrar los usuarios con más publicaciones para cada fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time_optimizacion_1(file_path: str):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date']).dt.date\n",
    "    \n",
    "    dataframe['username'] = [user.get('username', None) for user in dataframe['user']]\n",
    "    \n",
    "    dataframe = dataframe[['date', 'username']]\n",
    "    \n",
    "    grouped_data = dataframe.groupby(['date', 'username']).size().reset_index(name='tweet_count')\n",
    "    \n",
    "    top_10_dates = grouped_data.groupby('date')['tweet_count'].sum().nlargest(10)\n",
    "    \n",
    "    formatted_result = []\n",
    "    \n",
    "    for date in top_10_dates.index:\n",
    "        date_data = grouped_data[grouped_data['date'] == date]\n",
    "        top_user = date_data.loc[date_data['tweet_count'].idxmax()]\n",
    "        formatted_result.append((date, top_user['username']))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución: {execution_time} segundos\")\n",
    "    \n",
    "    return formatted_result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la prueba para verificar si el tiempo de ejecución disminuyó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 13.605211019515991 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time_optimizacion_1(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la función MEMORY:\n",
    "### def q1_memory(file_path: str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la prueba de memoria se utiliza el Script de Python ya que la librería memory-profile no se ejecuta adecuadamente con un Notebook "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función primaria es: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path: str):\n",
    "    \n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date'])\n",
    "    dataframe['date'] = dataframe['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    dataframe['username'] = dataframe['user'].apply(lambda x: x['username'])\n",
    "\n",
    "    filtered_columns = ['date', 'username']\n",
    "    dataframe = dataframe[filtered_columns]\n",
    "\n",
    "    grouped_data = dataframe.groupby(['date', 'username']).size().reset_index(name='tweet_count')\n",
    "    top_10_dates = grouped_data.groupby('date')['tweet_count'].sum().nlargest(10)\n",
    "\n",
    "\n",
    "    formatted_result = []\n",
    "\n",
    "    for date, tweet_count in top_10_dates.items():\n",
    "        date_data = grouped_data[grouped_data['date'] == date]\n",
    "        top_user = date_data.loc[date_data['tweet_count'].idxmax()]\n",
    "        formatted_result.append((datetime.datetime.strptime(date, '%Y-%m-%d').date(), top_user['username']))\n",
    "\n",
    "    return formatted_result\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos este resultado cuando aplicamos el script en Python como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import profile\n",
    "import pandas as pd\n",
    "\n",
    "@profile\n",
    "def q1_memory(file_path: str):\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date'])\n",
    "    dataframe['date'] = dataframe['date'].dt.strftime('%Y-%m-%d')\n",
    "    dataframe['username'] = dataframe['user'].apply(lambda x: x['username'])\n",
    "\n",
    "    filtered_columns = ['date', 'username']\n",
    "    dataframe = dataframe[filtered_columns]\n",
    "\n",
    "    grouped_data = dataframe.groupby(['date', 'username']).size().reset_index(name='tweet_count')\n",
    "    \n",
    "    del dataframe\n",
    "    \n",
    "    unique_dates = grouped_data['date'].unique()\n",
    "    top_10_dates = grouped_data[grouped_data['date'].isin(unique_dates[:10])]\n",
    "\n",
    "    formatted_result = []\n",
    "\n",
    "    for date in top_10_dates['date'].unique():\n",
    "        date_data = top_10_dates[top_10_dates['date'] == date]\n",
    "        top_user = date_data.loc[date_data['tweet_count'].idxmax()]\n",
    "        formatted_result.append((date, top_user['username']))\n",
    "\n",
    "\n",
    "    return formatted_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/Users/edwardguzman/Desktop/challenge_DE/farmers-protest-tweets-2021-2-4.json'\n",
    "    q1_memory(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de memoria es"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "     6     96.0 MiB     96.0 MiB           1       @profile\n",
    "     7                                             def q1_time(file_path: str):\n",
    "     8                                             \n",
    "     9   1614.6 MiB   1518.5 MiB           1       dataframe = pd.read_json(file_path, lines=True)\n",
    "    10                                         \n",
    "    11   1615.7 MiB      1.2 MiB           1       dataframe['date'] = pd.to_datetime(dataframe['date'])\n",
    "    12   1615.8 MiB      0.1 MiB           1       dataframe['date'] = dataframe['date'].dt.strftime('%Y-%m-%d')\n",
    "    13                                         \n",
    "    14   1615.8 MiB    -18.6 MiB      234815       dataframe['username'] = dataframe['user'].apply(lambda x: x['username'])\n",
    "    15                                         \n",
    "    16   1597.2 MiB    -18.7 MiB           1       filtered_columns = ['date', 'username']\n",
    "    17   1490.5 MiB   -106.7 MiB           1       dataframe = dataframe[filtered_columns]\n",
    "    18                                         \n",
    "    19   1449.8 MiB    -40.7 MiB           1       grouped_data = dataframe.groupby(['date', 'username']).size().reset_index(name='tweet_count')\n",
    "    20   1446.0 MiB     -3.9 MiB           1       top_10_dates = grouped_data.groupby('date')['tweet_count'].sum().nlargest(10)\n",
    "    21                                         \n",
    "    22                                         \n",
    "    23   1446.0 MiB      0.0 MiB           1       formatted_result = []\n",
    "    24                                         \n",
    "    25   1446.2 MiB    -19.7 MiB          11       for date, tweet_count in top_10_dates.items():\n",
    "    26   1446.1 MiB    -19.6 MiB          10       date_data = grouped_data[grouped_data['date'] == date]\n",
    "    27   1446.2 MiB    -18.8 MiB          10       top_user = date_data.loc[date_data['tweet_count'].idxmax()]\n",
    "    28   1446.2 MiB    -19.7 MiB          10       formatted_result.append((datetime.datetime.strptime(date, '%Y-%m-%d').date(), top_user['username']))\n",
    "    29                                         \n",
    "    30                                         \n",
    "    31                                         \n",
    "    32   1443.5 MiB     -2.7 MiB           1       return formatted_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos realizamos la optimización del codigo, en este caso eliminamos el dataframe luego de ser filtrado por último en lugar de calcular top_10_dates usando nlargest(10) en todo el DataFrame grouped_data, aplicaremos la operación solo a las fechas únicas en grouped_data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory_optimized(file_path: str):\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date']).dt.date\n",
    "    dataframe['username'] = dataframe['user'].apply(lambda x: x.get('username', None))\n",
    "\n",
    "    grouped_data = dataframe.groupby(['date', 'username']).size().reset_index(name='tweet_count')\n",
    "    del dataframe\n",
    "    top_10_dates = grouped_data.groupby('date')['tweet_count'].sum().nlargest(10).index\n",
    "\n",
    "    formatted_result = []\n",
    "\n",
    "    for date in top_10_dates:\n",
    "        date_data = grouped_data[grouped_data['date'] == date]\n",
    "        top_user = date_data.loc[date_data['tweet_count'].idxmax()]\n",
    "        formatted_result.append((date, top_user['username']))\n",
    "\n",
    "    return formatted_result\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "     4     96.0 MiB     96.0 MiB           1   @profile\n",
    "     5                                         def q1_time(file_path: str):\n",
    "     6   1565.4 MiB   1469.4 MiB           1   dataframe = pd.read_json(file_path, lines=True)\n",
    "     7                                         \n",
    "     8   1566.6 MiB      1.2 MiB           1   dataframe['date'] = pd.to_datetime(dataframe['date'])\n",
    "     9   1566.7 MiB      0.1 MiB           1   dataframe['date'] = dataframe['date'].dt.strftime('%Y-%m-%d')\n",
    "    10   1566.7 MiB    -18.7 MiB      234815   dataframe['username'] = dataframe['user'].apply(lambda x: x['username'])\n",
    "    11                                         \n",
    "    12   1548.0 MiB    -18.7 MiB           1   filtered_columns = ['date', 'username']\n",
    "    13   1474.8 MiB    -73.2 MiB           1   dataframe = dataframe[filtered_columns]\n",
    "    14                                         \n",
    "    15   1434.0 MiB    -40.8 MiB           1   grouped_data = dataframe.groupby(['date', 'username']).size().reset_index(name='tweet_count')\n",
    "    16                                             \n",
    "    17   1432.0 MiB     -2.0 MiB           1   del dataframe\n",
    "    18                                             \n",
    "    19   1430.0 MiB     -2.0 MiB           1   unique_dates = grouped_data['date'].unique()\n",
    "    20   1428.4 MiB     -1.6 MiB           1   top_10_dates = grouped_data[grouped_data['date'].isin(unique_dates[:10])]\n",
    "    21                                         \n",
    "    22   1428.4 MiB      0.0 MiB           1   formatted_result = []\n",
    "    23                                         \n",
    "    24   1428.6 MiB     -4.8 MiB          11   for date in top_10_dates['date'].unique():\n",
    "    25   1428.5 MiB     -4.7 MiB          10   date_data = top_10_dates[top_10_dates['date'] == date]\n",
    "    26   1428.6 MiB     -3.5 MiB          10   top_user = date_data.loc[date_data['tweet_count'].idxmax()]\n",
    "    27   1428.6 MiB     -4.8 MiB          10   formatted_result.append((date, top_user['username']))\n",
    "    28                                         \n",
    "    29                                         \n",
    "    30   1428.0 MiB     -0.6 MiB           1   return formatted_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEGUNDO PUNTO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos la base y la convertimos a un dataframe de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'farmers-protest-tweets-2021-2-4.json'\n",
    "dataframe = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos como primera medida el contenido de los tweets y creamos un diccionario para contar lo emojis cada vez que aparezcan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataframe['content']\n",
    "emoji_counts = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteramos ahora sobre cada tweet para encontrar todos los que son emojis utilizando expresiones regulares. Del mismo modo filtramos los emojis oara contar su frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in text:\n",
    "    emojis = re.findall(r'\\X', tweet)\n",
    "    for emoji_char in emojis:\n",
    "        if emoji_char in emoji.UNICODE_EMOJI['en']:\n",
    "            if emoji_char in emoji_counts:\n",
    "                emoji_counts[emoji_char] += 1\n",
    "            else:\n",
    "                emoji_counts[emoji_char] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un dataframe con el diccionario de emojis resultado de la iteración. Lo organizamos por conteo de los emojis y tomamos los primeros 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_df = pd.DataFrame(list(emoji_counts.items()), columns=['Emoji', 'Count'])\n",
    "emoji_df = emoji_df.sort_values(by='Count', ascending=False)\n",
    "top_10_emojis = emoji_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 emojis más utilizados:\n",
      "   Emoji  Count\n",
      "48     🙏   5049\n",
      "29     😂   3072\n",
      "0      🚜   2972\n",
      "1      🌾   2182\n",
      "6     🇮🇳   2086\n",
      "15     🤣   1668\n",
      "36     ✊   1642\n",
      "56    ❤️   1382\n",
      "11    🙏🏻   1317\n",
      "32     💚   1040\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 emojis más utilizados:\")\n",
    "print(top_10_emojis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa las manos juntas estan como un emoji diferente pero lo podemos unir por color tomado todos los colores. Este ejercicio se realiza tan solo para este emoji realizando una agrupación de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_praying_hands_emojis = ['🙏', '🙏🏻', '🙏🏼', '🙏🏽', '🙏🏾', '🙏🏿']\n",
    "\n",
    "for tweet in text:\n",
    "    emojis = re.findall(r'\\X', tweet)\n",
    "    for emoji_char in emojis:\n",
    "        if emoji_char in grouped_praying_hands_emojis:\n",
    "            emoji_base = '🙏'  \n",
    "            if emoji_base in emoji_counts:\n",
    "                emoji_counts[emoji_base] += 1\n",
    "            else:\n",
    "                emoji_counts[emoji_base] = 1\n",
    "        elif emoji_char in emoji.UNICODE_EMOJI['en']:\n",
    "            if emoji_char in emoji_counts:\n",
    "                emoji_counts[emoji_char] += 1\n",
    "            else:\n",
    "                emoji_counts[emoji_char] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la impresion de los resultados para verificar que no se cuenten como independientes sino como el mismo emoji las manos juntas, sin embargo esto puede suceder para todos los emojis que pueden tener cambio de color como la mayoría de manos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 emojis más utilizados:\n",
      "   Emoji  Count\n",
      "48     🙏  12335\n",
      "29     😂   6144\n",
      "0      🚜   5944\n",
      "1      🌾   4364\n",
      "6     🇮🇳   4172\n",
      "15     🤣   3336\n",
      "36     ✊   3284\n",
      "56    ❤️   2764\n",
      "32     💚   2080\n",
      "7      👇   1746\n"
     ]
    }
   ],
   "source": [
    "emoji_df = pd.DataFrame(list(emoji_counts.items()), columns=['Emoji', 'Count'])\n",
    "emoji_df = emoji_df.sort_values(by='Count', ascending=False)\n",
    "top_10_emojis = emoji_df.head(10)\n",
    "print(\"Top 10 emojis más utilizados:\")\n",
    "print(top_10_emojis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la función TIME:\n",
    "### def q2_time(file_path: str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unificamos todo el codigo anterior y creamos la función aplicando la librería time para medir el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re\n",
    "import time\n",
    "\n",
    "def q2_time(file_path: str):\n",
    "    start_time = time.time()\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    text = dataframe['content']\n",
    "    emoji_counts = {}\n",
    "    grouped_praying_hands_emojis = ['🙏', '🙏🏻', '🙏🏼', '🙏🏽', '🙏🏾', '🙏🏿']\n",
    "\n",
    "    for tweet in text:\n",
    "        emojis = re.findall(r'\\X', tweet)\n",
    "        for emoji_char in emojis:\n",
    "            if emoji_char in grouped_praying_hands_emojis:\n",
    "                emoji_base = '🙏'  \n",
    "                if emoji_base in emoji_counts:\n",
    "                    emoji_counts[emoji_base] += 1\n",
    "                else:\n",
    "                    emoji_counts[emoji_base] = 1\n",
    "            elif emoji_char in emoji.UNICODE_EMOJI['en']:\n",
    "                if emoji_char in emoji_counts:\n",
    "                    emoji_counts[emoji_char] += 1\n",
    "                else:\n",
    "                    emoji_counts[emoji_char] = 1\n",
    "\n",
    "    emoji_df = pd.DataFrame(list(emoji_counts.items()), columns=['Emoji', 'Count'])\n",
    "    emoji_df = emoji_df.sort_values(by='Count', ascending=False)\n",
    "    top_10_emojis = emoji_df.head(10)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución: {execution_time} segundos\")\n",
    "    \n",
    "    formatted_result = [(row['Emoji'], row['Count']) for _, row in top_10_emojis.iterrows()]\n",
    "    \n",
    "    return formatted_result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la ejecución de la función para verificar el Top 10 así como el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 76.3860969543457 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('🙏', 7286),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1642),\n",
       " ('❤️', 1382),\n",
       " ('💚', 1040),\n",
       " ('👇', 873)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la optimización para tener un menor tiempo de ejecución, para esto se uso un conjunto (set) para grouped_praying_hands_emojis en lugar de una lista ya que,  permite una búsqueda más eficiente de los emojis de manos juntas. Luego se simplificó la lógica de conteo de emojis utilizando el método .get() de los diccionarios de Python para obtener el recuento del emoji. Del mismo modo, se eliminó el uso de la variable text y se trabajó directamente con la columna 'content' del DataFrame. Po último, se utilizó el método nlargest() de Pandas para obtener los 10 emojis con el recuento más alto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re\n",
    "import time\n",
    "\n",
    "def q2_time_optimized(file_path: str):\n",
    "    start_time = time.time()\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    emoji_counts = {}\n",
    "    grouped_praying_hands_emojis = {'🙏', '🙏🏻', '🙏🏼', '🙏🏽', '🙏🏾', '🙏🏿'}\n",
    "\n",
    "    for tweet in dataframe['content']:\n",
    "        emojis = re.findall(r'\\X', tweet)\n",
    "        for emoji_char in emojis:\n",
    "            if emoji_char in grouped_praying_hands_emojis:\n",
    "                emoji_base = '🙏'  \n",
    "            elif emoji_char in emoji.UNICODE_EMOJI['en']:\n",
    "                emoji_base = emoji_char\n",
    "            else:\n",
    "                continue \n",
    "            emoji_counts[emoji_base] = emoji_counts.get(emoji_base, 0) + 1\n",
    "\n",
    "    emoji_df = pd.DataFrame(emoji_counts.items(), columns=['Emoji', 'Count'])\n",
    "    top_10_emojis = emoji_df.nlargest(10, 'Count')\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución: {execution_time} segundos\")\n",
    "    \n",
    "    formatted_result = [(row['Emoji'], row['Count']) for _, row in top_10_emojis.iterrows()]\n",
    "    \n",
    "    return formatted_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 72.37688517570496 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('🙏', 7286),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1642),\n",
       " ('❤️', 1382),\n",
       " ('💚', 1040),\n",
       " ('👇', 873)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time_optimized(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la función MEMORY:\n",
    "### def q2_memory(file_path: str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se parte del codigo base para verificar cual es el uso de memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re\n",
    "\n",
    "def q2_memory(file_path: str):\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "    text = dataframe['content']\n",
    "    emoji_counts = {}\n",
    "    grouped_praying_hands_emojis = ['🙏', '🙏🏻', '🙏🏼', '🙏🏽', '🙏🏾', '🙏🏿']\n",
    "\n",
    "    for tweet in text:\n",
    "        emojis = re.findall(r'\\X', tweet)\n",
    "        for emoji_char in emojis:\n",
    "            if emoji_char in grouped_praying_hands_emojis:\n",
    "                emoji_base = '🙏'  \n",
    "                if emoji_base in emoji_counts:\n",
    "                    emoji_counts[emoji_base] += 1\n",
    "                else:\n",
    "                    emoji_counts[emoji_base] = 1\n",
    "            elif emoji_char in emoji.UNICODE_EMOJI['en']:\n",
    "                if emoji_char in emoji_counts:\n",
    "                    emoji_counts[emoji_char] += 1\n",
    "                else:\n",
    "                    emoji_counts[emoji_char] = 1\n",
    "\n",
    "    emoji_df = pd.DataFrame(list(emoji_counts.items()), columns=['Emoji', 'Count'])\n",
    "    emoji_df = emoji_df.sort_values(by='Count', ascending=False)\n",
    "    top_10_emojis = emoji_df.head(10)\n",
    "    \n",
    "    formatted_result = [(row['Emoji'], row['Count']) for _, row in top_10_emojis.iterrows()]\n",
    "    \n",
    "    return formatted_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 7286),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1642),\n",
       " ('❤️', 1382),\n",
       " ('💚', 1040),\n",
       " ('👇', 873)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para optimizar el uso de memoria para este codigo lo primero que se realiza es utilizar operaciones vectorizadas de Pandas para que en vez de iterar sobre cada fila de dataframe['content'] para contar los emojis, se podría utilizar la operación sobre toda la columna. Lo segundo es eliminar las variables innecesarias después de calcular los recuentos de emoji y crear el DataFrame se eliminan kas variables temporales. del mismo modo se usan estructuras de datos más eficientes para almacenar y procesar los recuentos de emoji en lugar de un diccionario de Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re\n",
    "\n",
    "def q2_memory_optimized(file_path: str):\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "    text = dataframe['content']\n",
    "    emoji_counts = {}\n",
    "\n",
    "    for tweet in text:\n",
    "        emojis = re.findall(r'\\X', tweet)\n",
    "        for emoji_char in emojis:\n",
    "            if emoji_char in emoji.UNICODE_EMOJI['en']:\n",
    "                if emoji_char in emoji_counts:\n",
    "                    emoji_counts[emoji_char] += 1\n",
    "                else:\n",
    "                    emoji_counts[emoji_char] = 1\n",
    "\n",
    "    emoji_df = pd.DataFrame(list(emoji_counts.items()), columns=['Emoji', 'Count'])\n",
    "    top_10_emojis = emoji_df.sort_values(by='Count', ascending=False).head(10)\n",
    "\n",
    "    formatted_result = [(row['Emoji'], row['Count']) for _, row in top_10_emojis.iterrows()]\n",
    "    \n",
    "    return formatted_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1642),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory_optimized(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERCER PUNTO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo tomamos el contenido del tweet e inicializamos un diccionario para contar las menciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = dataframe['content']\n",
    "mention_counts = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora iteramos sobre cada item del tweet y encontramos todas las menciones (@) en el tweet utilizando expresiones regulares así mismo las filtramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    mentions = re.findall(r'@(\\w+)', tweet)\n",
    "    for mention in mentions:\n",
    "        if mention in mention_counts:\n",
    "            mention_counts[mention] += 1\n",
    "        else:\n",
    "            mention_counts[mention] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora convertimos el diccionario de las menciones aun dataframe y lo organizamos tomando los 10 de mayor conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_df = pd.DataFrame(list(mention_counts.items()), columns=['Usuario', 'Conteo'])\n",
    "top_10_mencionados = mention_df.sort_values(by='Conteo', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos lo resultados para verificar que este funcionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 usuarios más mencionados:\n",
      "             Usuario  Conteo\n",
      "0       narendramodi    2261\n",
      "2    Kisanektamorcha    1836\n",
      "56   RakeshTikaitBKU    1639\n",
      "9           PMOIndia    1422\n",
      "84       RahulGandhi    1125\n",
      "188    GretaThunberg    1046\n",
      "321      RaviSinghKA    1015\n",
      "444          rihanna     972\n",
      "147    UNHumanRights     962\n",
      "187      meenaharris     925\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 usuarios más mencionados:\")\n",
    "print(top_10_mencionados)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la función TIME:\n",
    "### def q3_time(file_path: str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def q3_time(file_path: str):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "    tweets = dataframe['content']\n",
    "    mention_counts = {}\n",
    "\n",
    "    for tweet in tweets:\n",
    "        mentions = re.findall(r'@(\\w+)', tweet)\n",
    "        for mention in mentions:\n",
    "            if mention in mention_counts:\n",
    "                mention_counts[mention] += 1\n",
    "            else:\n",
    "                mention_counts[mention] = 1\n",
    "\n",
    "\n",
    "    mention_df = pd.DataFrame(list(mention_counts.items()), columns=['User', 'Count'])\n",
    "    top_10_mencionados = mention_df.sort_values(by='Count', ascending=False).head(10)\n",
    "    formatted_result = [(row['User'], row['Count']) for _, row in top_10_mencionados.iterrows()]\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución: {execution_time} segundos\")\n",
    "\n",
    "    return formatted_result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 44.93638777732849 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2261),\n",
       " ('Kisanektamorcha', 1836),\n",
       " ('RakeshTikaitBKU', 1639),\n",
       " ('PMOIndia', 1422),\n",
       " ('RahulGandhi', 1125),\n",
       " ('GretaThunberg', 1046),\n",
       " ('RaviSinghKA', 1015),\n",
       " ('rihanna', 972),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 925)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para optimizar la función utilizaremos collections.Counter en lugar de mantener un diccionario mention_counts y realizar comprobaciones de pertenencia en cada iteración. Tambien se puede evitar la creación innecesaria de un DataFrame completo y por último podemos usar Counter.most_common(10) para obtener directamente las 10 menciones más comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def q3_time_optimized(file_path: str):\n",
    "    start_time = time.time()\n",
    "\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "    tweets = dataframe['content']\n",
    "    \n",
    "    mention_counts = Counter()\n",
    "    for tweet in tweets:\n",
    "        mentions = re.findall(r'@(\\w+)', tweet)\n",
    "        mention_counts.update(mentions)\n",
    "    \n",
    "    top_10_mencionados = mention_counts.most_common(10)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución: {execution_time} segundos\")\n",
    "\n",
    "    return top_10_mencionados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 37.032829999923706 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2261),\n",
       " ('Kisanektamorcha', 1836),\n",
       " ('RakeshTikaitBKU', 1639),\n",
       " ('PMOIndia', 1422),\n",
       " ('RahulGandhi', 1125),\n",
       " ('GretaThunberg', 1046),\n",
       " ('RaviSinghKA', 1015),\n",
       " ('rihanna', 972),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 925)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time_optimized(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la función Memory:\n",
    "### def q3_memory(file_path: str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos de nuevo de la función original para poder optimizarla con respecto al uso de la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import profile\n",
    "\n",
    "@profile\n",
    "def q3_memory(file_path: str):\n",
    "    \n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "    tweets = dataframe['content']\n",
    "    mention_counts = {}\n",
    "\n",
    "    for tweet in tweets:\n",
    "        mentions = re.findall(r'@(\\w+)', tweet)\n",
    "        for mention in mentions:\n",
    "            if mention in mention_counts:\n",
    "                mention_counts[mention] += 1\n",
    "            else:\n",
    "                mention_counts[mention] = 1\n",
    "\n",
    "\n",
    "    mention_df = pd.DataFrame(list(mention_counts.items()), columns=['User', 'Count'])\n",
    "    top_10_mencionados = mention_df.sort_values(by='Count', ascending=False).head(10)\n",
    "    formatted_result = [(row['User'], row['Count']) for _, row in top_10_mencionados.iterrows()]\n",
    "      \n",
    "\n",
    "    return formatted_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/Users/edwardguzman/Desktop/challenge_DE/farmers-protest-tweets-2021-2-4.json'\n",
    "    result = q3_memory(file_path)\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "     5     98.4 MiB     98.4 MiB           1   @profile\n",
    "     6                                         def q3_memory(file_path: str):\n",
    "     7                                           \n",
    "     8                                         \n",
    "     9   1550.7 MiB   1452.3 MiB           1   dataframe = pd.read_json(file_path, lines=True)\n",
    "    10   1550.7 MiB      0.0 MiB           1   tweets = dataframe['content']\n",
    "    11   1550.7 MiB      0.0 MiB           1   mention_counts = {}\n",
    "    12                                         \n",
    "    13   1551.0 MiB -86270.1 MiB      117408   for tweet in tweets:\n",
    "    14   1551.0 MiB -86268.7 MiB      117407   mentions = re.findall(r'@(\\w+)', tweet)\n",
    "    15   1551.0 MiB -162634.9 MiB      221473  for mention in mentions:\n",
    "    16   1551.0 MiB -76364.8 MiB      104066   if mention in mention_counts:\n",
    "    17   1551.0 MiB -66847.7 MiB       88391   mention_counts[mention] += 1\n",
    "    18                                         else:\n",
    "    19   1551.0 MiB  -9518.1 MiB       15675   mention_counts[mention] = 1\n",
    "    20                                         \n",
    "    21                                         \n",
    "    22   1530.1 MiB    -20.9 MiB           1   mention_df = pd.DataFrame(list(mention_counts.items()), columns=['User', 'Count'])\n",
    "    23   1530.5 MiB      0.5 MiB           1   top_10_mencionados = mention_df.sort_values(by='Count', ascending=False).head(10)\n",
    "    24   1530.5 MiB      0.0 MiB          13   formatted_result = [(row['User'], row['Count']) for _, row in top_10_mencionados.iterrows()]\n",
    "    25                                             \n",
    "    26   1530.5 MiB      0.0 MiB           1   return formatted_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos la optimización para revisar si tenemos un mejor rendimiento de memoria. Evitamos la creación de un diccionario mention_counts para almacenar todas las menciones y sus recuentos y mas bien utilizamos collections.Counter para contar las menciones directamente mientras iteramos a través de los tweets. Además, podemos usar Counter.most_common(10) para obtener directamente las 10 menciones más comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def q3_memory_optimized(file_path: str):\n",
    "\n",
    "    dataframe = pd.read_json(file_path, lines=True)\n",
    "    tweets = dataframe['content']\n",
    "    \n",
    "    mention_counts = Counter()\n",
    "    for tweet in tweets:\n",
    "        mentions = re.findall(r'@(\\w+)', tweet)\n",
    "        mention_counts.update(mentions)\n",
    "    \n",
    "    top_10_mencionados = mention_counts.most_common(10)\n",
    "    \n",
    "    return top_10_mencionados\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "     7     98.3 MiB     98.3 MiB           1   @profile\n",
    "     8                                         def q3_memory_optimized(file_path: str):\n",
    "     9                                             \n",
    "    10                                         \n",
    "    11   1512.5 MiB   1464.1 MiB           1       dataframe = pd.read_json(file_path, lines=True)\n",
    "    12   1512.5 MiB      0.0 MiB           1       tweets = dataframe['content']\n",
    "    13   1512.5 MiB      0.0 MiB           1       mention_counts = Counter()\n",
    "    14                                         \n",
    "    15   1512.7 MiB -86270.1 MiB      117408       for tweet in tweets:\n",
    "    16   1512.7 MiB -86268.7 MiB      117407           mentions = re.findall(r'@(\\w+)', tweet)\n",
    "    17   1512.7 MiB -86269.9 MiB      117407           mention_counts.update(mentions)\n",
    "    18                                             \n",
    "    19   1501.5 MiB     -1.2 MiB           1       top_10_mencionados = mention_counts.most_common(10)\n",
    "    20                                             \n",
    "    21                                             \n",
    "    22   1501.5 MiB      0.0 MiB           1       return top_10_mencionados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
